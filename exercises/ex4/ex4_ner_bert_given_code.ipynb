{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2025-Tutorial-Notebooks/blob/main/exercises/ex4/ex4_ner_bert_given_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBK3YDwgVBjN"
      },
      "source": [
        "# Load and prepare the required data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqqgMPaTyVsT",
        "outputId": "9f08e9f1-7954-4de6-8e4f-49fb2cfd0b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in ./.venv312/lib64/python3.12/site-packages (4.4.1)\n",
            "Requirement already satisfied: filelock in ./.venv312/lib64/python3.12/site-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv312/lib64/python3.12/site-packages (from datasets) (2.3.4)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv312/lib64/python3.12/site-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv312/lib64/python3.12/site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in ./.venv312/lib64/python3.12/site-packages (from datasets) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in ./.venv312/lib64/python3.12/site-packages (from datasets) (2.32.5)\n",
            "Requirement already satisfied: httpx<1.0.0 in ./.venv312/lib64/python3.12/site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in ./.venv312/lib64/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in ./.venv312/lib64/python3.12/site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in ./.venv312/lib64/python3.12/site-packages (from datasets) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in ./.venv312/lib64/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.9.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in ./.venv312/lib64/python3.12/site-packages (from datasets) (0.25.2)\n",
            "Requirement already satisfied: packaging in ./.venv312/lib64/python3.12/site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv312/lib64/python3.12/site-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv312/lib64/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in ./.venv312/lib64/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in ./.venv312/lib64/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in ./.venv312/lib64/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in ./.venv312/lib64/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in ./.venv312/lib64/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv312/lib64/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv312/lib64/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv312/lib64/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv312/lib64/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv312/lib64/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv312/lib64/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./.venv312/lib64/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv312/lib64/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv312/lib64/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv312/lib64/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in ./.venv312/lib64/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv312/lib64/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv312/lib64/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv312/lib64/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv312/lib64/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-WV5IWPkxmjo"
      },
      "outputs": [],
      "source": [
        "# Choose a supported language, apart from English. Examples: \"de\", \"fr\", \"es\", \"it\".\n",
        "# NOTE: See dataset card for supported languages (https://huggingface.co/datasets/unimelb-nlp/wikiann)\n",
        "chosen_language_code = \"de\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ggqfgV_nO5Qj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/work/Documents/GitHub/ML4NLP1/exercises/ex4/.venv312/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "# NOTE: If the maximum sequence length exceeds the model's maximum\n",
        "# sequence length, you need to make adjustments (for example, when\n",
        "# choosing 'en')\n",
        "test_set = datasets.load_dataset(\"unimelb-nlp/wikiann\", chosen_language_code, split=\"test[:2000]\")\n",
        "train_set1000 = datasets.load_dataset(\"unimelb-nlp/wikiann\", chosen_language_code, split=\"train[:1000]\")\n",
        "train_set3000 = datasets.load_dataset(\"unimelb-nlp/wikiann\", chosen_language_code, split=\"train[:3000]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNYxEU3YO5Ql"
      },
      "source": [
        "**NOTE: Make sure that there are indeed as many data points in the above sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IMFy3wCLO5Qm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
            "    num_rows: 3000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
            "    num_rows: 2000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_set1000)\n",
        "print(train_set3000)\n",
        "print(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N0wMdjnvO5Qn"
      },
      "outputs": [],
      "source": [
        "ner_tags = {\n",
        "    \"O\": 0,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-PER\": 2,\n",
        "    \"B-ORG\": 3,\n",
        "    \"I-ORG\": 4,\n",
        "    \"B-LOC\": 5,\n",
        "    \"I-LOC\": 6\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ArOWU99T58D"
      },
      "source": [
        "**TODO: Inspect and Describe the Data, including Average and Maximum Input length (in tokens)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last entry: {'tokens': ['Georg', 'Franz', 'August', 'von', 'Buquoy'], 'ner_tags': [1, 2, 2, 2, 2], 'langs': ['de', 'de', 'de', 'de', 'de'], 'spans': ['PER: Georg Franz August von Buquoy']}\n",
            "Average length: 9.767\n",
            "Max length: 76\n"
          ]
        }
      ],
      "source": [
        "#train_set1000\n",
        "lengths = [len(entry[\"tokens\"]) for entry in train_set1000]\n",
        "\n",
        "avg_length = sum(lengths) / len(lengths)\n",
        "max_length = max(lengths)\n",
        "print(\"Last entry:\", train_set1000[-1])\n",
        "print(\"Average length:\", avg_length)\n",
        "print(\"Max length:\", max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First entry: {'tokens': ['Mechanorezeptoren', \"''\", 'werden', 'durch', 'mechanische', 'Reize', 'angesprochen', '.'], 'ner_tags': [5, 0, 0, 0, 0, 0, 0, 0], 'langs': ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de'], 'spans': ['LOC: Mechanorezeptoren']}\n",
            "Average length: 9.738666666666667\n",
            "Max length: 76\n"
          ]
        }
      ],
      "source": [
        "#train_set3000\n",
        "lengths = [len(entry[\"tokens\"]) for entry in train_set3000]\n",
        "\n",
        "avg_length = sum(lengths) / len(lengths)\n",
        "max_length = max(lengths)\n",
        "print(\"First entry:\", train_set3000[-1])\n",
        "print(\"Average length:\", avg_length)\n",
        "print(\"Max length:\", max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First entry: {'tokens': ['Schatzhaus', 'der', 'Athener', 'in', 'Delphi'], 'ner_tags': [3, 4, 4, 0, 5], 'langs': ['de', 'de', 'de', 'de', 'de'], 'spans': ['ORG: Schatzhaus der Athener', 'LOC: Delphi']}\n",
            "Average length: 9.6325\n",
            "Max length: 45\n"
          ]
        }
      ],
      "source": [
        "#train_set3000\n",
        "lengths = [len(entry[\"tokens\"]) for entry in test_set]\n",
        "\n",
        "avg_length = sum(lengths) / len(lengths)\n",
        "max_length = max(lengths)\n",
        "print(\"First entry:\", test_set[-1])\n",
        "print(\"Average length:\", avg_length)\n",
        "print(\"Max length:\", max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfyYXRLoU6mV"
      },
      "source": [
        "üìù‚ùìWhy do you need to be aware of the longest input length within your dataset? Which parameter of the model dictates this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n-1XQS_M2WO2"
      },
      "outputs": [],
      "source": [
        "# TODO: Adjust by actually finding the maximum sequence length\n",
        "max_sequence_length = 76"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "idpa54l4O5Qv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76\n"
          ]
        }
      ],
      "source": [
        "print(max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers: 4.41.1\n",
            "huggingface_hub: 0.25.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import transformers, huggingface_hub\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"huggingface_hub:\", huggingface_hub.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5KxkH5vBO5Qw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded!\n",
            "Vocabulary size: 30000\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# TODO: Load the tokenizer\n",
        "model_name = \"google-bert/bert-base-german-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"Tokenizer loaded!\")\n",
        "print(\"Vocabulary size:\", tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2qFayooO5Qx"
      },
      "source": [
        "üìù‚ùìThe dataset is split into words, and the assigned labels are for words. How should we deal with labels **after** tokenization? NOTE: Each word may be split into one or multiple tokens by the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mvc66dR9x6Fe"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement this function\n",
        "def encode_and_align_labels(dataset, tokenizer, max_sequence_length):\n",
        "    \"\"\"Tokenizes the input tokens and aligns the word-level NER labels with the tokenized output.\"\"\"\n",
        "    # policy: only the first sub-token gets the word's label; others -> -100\n",
        "    label_all_tokens = False\n",
        "\n",
        "    if not getattr(tokenizer, \"is_fast\", False):\n",
        "        raise ValueError(\n",
        "            \"This function requires a *fast* tokenizer (tokenizers library) \"\n",
        "            \"because it uses `word_ids()` to align labels.\"\n",
        "        )\n",
        "\n",
        "    def _process(example):\n",
        "        # example[\"tokens\"] is a list[str], example[\"ner_tags\"] is a list[int] (one per word)\n",
        "        enc = tokenizer(\n",
        "            example[\"tokens\"],\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            max_length=max_sequence_length,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        word_ids = enc.word_ids()  # len == max_sequence_length (after padding)\n",
        "        labels = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # special token (CLS/SEP/PAD) -> ignore in loss\n",
        "                labels.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # first token of a word -> take that word's label\n",
        "                labels.append(example[\"ner_tags\"][word_idx])\n",
        "            else:\n",
        "                # subsequent sub-token of the same word\n",
        "                if label_all_tokens:\n",
        "                    labels.append(example[\"ner_tags\"][word_idx])\n",
        "                else:\n",
        "                    labels.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        enc[\"labels\"] = labels\n",
        "        return enc\n",
        "\n",
        "    # map over the whole dataset; remove original columns to keep only model inputs\n",
        "    cols_to_remove = [c for c in dataset.column_names if c not in (\"id\",)]\n",
        "    tokenized = dataset.map(_process, remove_columns=cols_to_remove)\n",
        "    return tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_0zufGtNw6yk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 4921.78 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 4881.46 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:00<00:00, 5216.59 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Encode the two training sets and the test set by applying the function above\n",
        "encoded_test_set = encode_and_align_labels(test_set,tokenizer,max_sequence_length)\n",
        "encoded_train_set1000 = encode_and_align_labels(train_set1000,tokenizer,max_sequence_length)\n",
        "encoded_train_set3000 = encode_and_align_labels(train_set3000,tokenizer,max_sequence_length)\n",
        "\n",
        "\n",
        "\n",
        "# Set format for PyTorch\n",
        "encoded_test_set.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")\n",
        "encoded_train_set1000.set_format(\n",
        "\ttype=\"torch\",\n",
        "\tcolumns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")\n",
        "encoded_train_set3000.set_format(\n",
        "\ttype=\"torch\",\n",
        "\tcolumns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oykr25qgnBOg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_42291/1694782954.py:5: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
            "  labels = np.concatenate([np.array(x[\"labels\"]) for x in ds])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train1000 labels: {'total_tokens': 76000, 'ignored_-100': 66258, 'ignored_%': 87.18157894736842, 'num_labeled_tokens': 9742, 'min_label_id': 0, 'max_label_id': 6}\n",
            "train3000 labels: {'total_tokens': 228000, 'ignored_-100': 198811, 'ignored_%': 87.19780701754387, 'num_labeled_tokens': 29189, 'min_label_id': 0, 'max_label_id': 6}\n",
            "test labels: {'total_tokens': 152000, 'ignored_-100': 132735, 'ignored_%': 87.32565789473684, 'num_labeled_tokens': 19265, 'min_label_id': 0, 'max_label_id': 6}\n",
            "\n",
            "train1000[0]\n",
            "input_ids: (76,)\n",
            "attention_mask: (76,)\n",
            "labels: (76,)\n",
            "\n",
            "train3000[0]\n",
            "input_ids: (76,)\n",
            "attention_mask: (76,)\n",
            "labels: (76,)\n",
            "\n",
            "test[0]\n",
            "input_ids: (76,)\n",
            "attention_mask: (76,)\n",
            "labels: (76,)\n",
            "train1000 specials: {'special_tokens_with_non_ignored_labels': 0, 'checked_pairs': 76000}\n",
            "train3000 specials: {'special_tokens_with_non_ignored_labels': 0, 'checked_pairs': 228000}\n",
            "test specials: {'special_tokens_with_non_ignored_labels': 0, 'checked_pairs': 152000}\n"
          ]
        }
      ],
      "source": [
        "# Check out how the training sets are encoded\n",
        "import numpy as np\n",
        "\n",
        "def label_stats(ds):\n",
        "    labels = np.concatenate([np.array(x[\"labels\"]) for x in ds])\n",
        "    return {\n",
        "        \"total_tokens\": int(labels.size),\n",
        "        \"ignored_-100\": int((labels == -100).sum()),\n",
        "        \"ignored_%\":    float((labels == -100).mean() * 100),\n",
        "        \"num_labeled_tokens\": int((labels != -100).sum()),\n",
        "        \"min_label_id\": int(labels[labels != -100].min()) if (labels != -100).any() else None,\n",
        "        \"max_label_id\": int(labels[labels != -100].max()) if (labels != -100).any() else None,\n",
        "    }\n",
        "\n",
        "print(\"train1000 labels:\", label_stats(encoded_train_set1000))\n",
        "print(\"train3000 labels:\", label_stats(encoded_train_set3000))\n",
        "print(\"test labels:\",      label_stats(encoded_test_set))\n",
        "\n",
        "def show_shapes(ds, name):\n",
        "    print(f\"\\n{name}\")\n",
        "    ex = ds[0]\n",
        "    for k, v in ex.items():\n",
        "        if hasattr(v, \"size\"):\n",
        "            print(f\"{k}: {tuple(v.size())}\")\n",
        "        else:\n",
        "            print(f\"{k}: (scalar or list) -> {type(v)}\")\n",
        "\n",
        "show_shapes(encoded_train_set1000, \"train1000[0]\")\n",
        "show_shapes(encoded_train_set3000, \"train3000[0]\")\n",
        "show_shapes(encoded_test_set,      \"test[0]\")\n",
        "\n",
        "def special_token_label_check(ds, tokenizer):\n",
        "    cls_id = tokenizer.cls_token_id\n",
        "    sep_id = tokenizer.sep_token_id\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    bad = 0\n",
        "    total = 0\n",
        "    for row in ds:\n",
        "        ids = row[\"input_ids\"]\n",
        "        labs = row[\"labels\"]\n",
        "        for tid, lab in zip(ids, labs):\n",
        "            if tid in (cls_id, sep_id, pad_id) and lab != -100:\n",
        "                bad += 1\n",
        "            total += 1\n",
        "    return {\"special_tokens_with_non_ignored_labels\": bad, \"checked_pairs\": total}\n",
        "\n",
        "print(\"train1000 specials:\", special_token_label_check(encoded_train_set1000, tokenizer))\n",
        "print(\"train3000 specials:\", special_token_label_check(encoded_train_set3000, tokenizer))\n",
        "print(\"test specials:\",      special_token_label_check(encoded_test_set,      tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEJXYvbOa-E4"
      },
      "source": [
        "Example of how your output could look like.\n",
        "\n",
        "input_ids: torch.Size([???])\n",
        "\n",
        "token_type_ids: torch.Size([???])\n",
        "\n",
        "attention_mask: torch.Size([???])\n",
        "\n",
        "labels: torch.Size([???])\n",
        "\n",
        "üìù‚ùìWhat value should replace the three question marks in your print? Should this be the sample for all samples? Why/Why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnWH-MaKO5Q1"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdIPhpLiO5Q1"
      },
      "source": [
        "## Training Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy2LxB_2O5Q2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSpxJMtkO5Q3"
      },
      "source": [
        "**TODO: Complete the following, reusable functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKhJH6wQO5Q4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(preds):\n",
        "    \"\"\"\n",
        "    Compute macro and micro F1 scores from PredictionOutput\n",
        "\n",
        "    Args:\n",
        "        preds: transformers.trainer_utils.PredictionOutput\n",
        "\n",
        "    Returns:\n",
        "        dict with macro_f1 and micro_f1 scores\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePcLuf4QO5Q4"
      },
      "outputs": [],
      "source": [
        "def freeze_weights(model):\n",
        "    \"\"\"Freeze the weights for a given model.\n",
        "\n",
        "    Args:\n",
        "        model: transformers.PreTrainedModel\n",
        "\n",
        "    Returns:\n",
        "\t\t\tmodel: transformers.PreTrainedModel\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cevovk8ZO5Q5"
      },
      "source": [
        "## Variation 1: 1000 sentences, no frozen weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru--p5NZO5Q6"
      },
      "source": [
        "**TODO: Initialise your model and set up your training arguments**\n",
        "\n",
        "üìù‚ùìWhen initializing the BertForTokenClassification-class with BERT-base you should get a warning message. Explain why you get this message.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD72g8GC2Rwx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_DBflx1wzRa"
      },
      "source": [
        "**TODO: Train your Model ‚ö° GPU 2-3 mins**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3XEgVHrxJTq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt86VaDIiMWd"
      },
      "source": [
        "**TODO: Compute Metrics/Performance of your model.**\n",
        "\n",
        "üìù‚ùì Is there a challenge when evaluating the predictions of your model? Why is this challenge present and how do you plan to deal with it?\n",
        "\n",
        "Hint: Look at the lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ryi-Vnrw7NJ"
      },
      "source": [
        "To avoid rerunning, please also print the metrics of each model that completed training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nE3HeKW4TFl"
      },
      "outputs": [],
      "source": [
        "# print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AKh4cXm7F8R"
      },
      "source": [
        "## Variant 2: 3000 sentences, no frozen weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHmo4lCp2Bj7"
      },
      "outputs": [],
      "source": [
        "# Repeat after each run to save VRAM\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsU1E0_p7ITW"
      },
      "source": [
        "## Variant 3: 1000 sentences, frozen weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTXRQCgz7Vcu"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6wE6mAm7y7m"
      },
      "source": [
        "## Variant 4: 3000 sentences, frozen weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxkAuigt7y7n"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iTcwgmtO5Q_"
      },
      "source": [
        "# Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2LyNOP6oVsz"
      },
      "source": [
        "üìù‚ùì Template:\n",
        "\n",
        "Summary of Performance of the four Model Variants\n",
        "\n",
        "1. Whole Model finetuning, 1000 samples:\n",
        "2. Whole Model finetuning, 3000 samples:\n",
        "3. Frozen Backbone, 1000 samples:\n",
        "4. Frozen Backbone 3000 samples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPi_aox2O5RA"
      },
      "source": [
        "üìù‚ùì When we freeze the transformer backbone weights, which weights are being tuned during fine-tuning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR2j40qivASb"
      },
      "source": [
        "üìù‚ùì Are there differences between f1-micro and f1-macro score? If so, why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqyOEWsr-J4s"
      },
      "source": [
        "üìù‚ùì Is it better to freeze or not to freeze the transformer backbone weights? Hypothesize why"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBCHlRNgxPTn"
      },
      "source": [
        "\n",
        "\n",
        "üìù‚ùì Write your lab report here addressing all questions in the notebook"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
