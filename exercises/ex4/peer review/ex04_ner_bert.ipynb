{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2025-Tutorial-Notebooks/blob/main/exercises/ex4/ex4_ner_bert_given_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBK3YDwgVBjN"
   },
   "source": [
    "# Load and prepare the required data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17880,
     "status": "ok",
     "timestamp": 1763317237963,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "YqqgMPaTyVsT",
    "outputId": "89e2261e-ad39-4785-96ac-1e4347c17c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1763317238076,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "-WV5IWPkxmjo"
   },
   "outputs": [],
   "source": [
    "# Choose a supported language, apart from English. Examples: \"de\", \"fr\", \"es\", \"it\".\n",
    "# NOTE: See dataset card for supported languages (https://huggingface.co/datasets/unimelb-nlp/wikiann)\n",
    "chosen_language_code = \"es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16670,
     "status": "ok",
     "timestamp": 1763317254751,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "ggqfgV_nO5Qj",
    "outputId": "573ebf7d-c8c1-4c32-e194-734734d1eb86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# NOTE: If the maximum sequence length exceeds the model's maximum\n",
    "# sequence length, you need to make adjustments (for example, when\n",
    "# choosing 'en')\n",
    "test_set = datasets.load_dataset(\"unimelb-nlp/wikiann\", chosen_language_code, split=\"test[:2000]\")\n",
    "\n",
    "# Creation of randomized training subsets\n",
    "raw_train = datasets.load_dataset(\"unimelb-nlp/wikiann\", chosen_language_code, split=\"train\")\n",
    "train_shuffled = raw_train.shuffle(seed=42) # for reproducible random subsets\n",
    "\n",
    "train_set1000 = train_shuffled.select(range(1000))\n",
    "train_set3000 = train_shuffled.select(range(3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNYxEU3YO5Ql"
   },
   "source": [
    "**NOTE: Make sure that there are indeed as many data points in the above sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1763317254760,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "IMFy3wCLO5Qm",
    "outputId": "1a3416d9-6832-4cf2-d967-d33c2951562c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "    num_rows: 3000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_set1000)\n",
    "print(train_set3000)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1763317254782,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "N0wMdjnvO5Qn"
   },
   "outputs": [],
   "source": [
    "ner_tags = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ArOWU99T58D"
   },
   "source": [
    "**TODO: Inspect and Describe the Data, including Average and Maximum Input length (in tokens)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfyYXRLoU6mV"
   },
   "source": [
    "üìù‚ùìWhy do you need to be aware of the longest input length within your dataset? Which parameter of the model dictates this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz2DJ-L8R4TB"
   },
   "source": [
    "\n",
    " We must be aware of the longest input length because BERT has a fixed maximum context window. Anything longer must be truncated (information loss) or split across chunks. The limit is set by the model‚Äôs configuration parameter `config.max_position_embeddings` (exposed via the tokenizer as `tokenizer.model_max_length`). Choosing an appropriate `max_sequence_length` ensures that we do not exceed this limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16586,
     "status": "ok",
     "timestamp": 1763317271370,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "VPDulIpeFNYI",
    "outputId": "2aef0d89-2e8e-4fc8-97f6-2f6d97b8908b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "tokenizer.model_max_length: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# TODO: Load the tokenizer\n",
    "model_checkpoint = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "print(\"Tokenizer loaded.\")\n",
    "print(\"tokenizer.model_max_length:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1587,
     "status": "ok",
     "timestamp": 1763317272960,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "3GzhDLsdFc_j",
    "outputId": "ed0a3444-1e63-4bbc-d1e5-42d2ef3e21a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Token Length Statistics ===\n",
      "Max token length:          104\n",
      "Avg token length:          13.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def analyze_token_lengths(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    This function computes maximum and average token lengths and returns a dictionary.\n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "\n",
    "    for example in dataset:\n",
    "        text = \" \".join(example[\"tokens\"])\n",
    "        encoded = tokenizer(text, add_special_tokens=True)\n",
    "        lengths.append(len(encoded[\"input_ids\"]))\n",
    "\n",
    "    lengths = np.array(lengths)\n",
    "\n",
    "    stats = {\n",
    "        \"avg_len\": round(float(np.mean(lengths))),\n",
    "        \"max_len\": int(np.max(lengths))\n",
    "    }\n",
    "\n",
    "    print(\"\\n\\n=== Token Length Statistics ===\")\n",
    "    print(f\"Max token length:          {stats['max_len']}\")\n",
    "    print(f\"Avg token length:          {stats['avg_len']:.2f}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "sub_stats = analyze_token_lengths(train_set3000, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1763317272974,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "8-Wnb9bTGgew",
    "outputId": "dd124283-e862-4287-e358-387e26bea954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = sub_stats[\"max_len\"]\n",
    "print(max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763317272977,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "J6wEBfRF_whR"
   },
   "outputs": [],
   "source": [
    "# TODO: Adjust by actually finding the maximum sequence length\n",
    "max_sequence_length = 128\n",
    "\n",
    "# Note: We chose a maximum sequence length of 128 instead of 104 which is bigger than 104 and comply with the common BERT NER training practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2qFayooO5Qx"
   },
   "source": [
    "üìù‚ùìThe dataset is split into words, and the assigned labels are for words. How should we deal with labels **after** tokenization? NOTE: Each word may be split into one or multiple tokens by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WGavFS5HkJh"
   },
   "source": [
    "\n",
    "The dataset labels are at word-level. After tokenization, each word might produce multiple subword tokens. The standard and simplest approach for BertForTokenClassification is:\n",
    "\n",
    "1. Assign the original word label to the first subword token of that word.\n",
    "2. Assign -100 to all subsequent subword pieces and special tokens ([CLS], [SEP]).\n",
    "    - -100 is the default ignore index in PyTorch‚Äôs CrossEntropyLoss, so those positions don‚Äôt affect the loss (they are ignored).\n",
    "\n",
    "This preserves alignment while ensuring the loss is computed once per original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1763317272982,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "Ps95w9YuXYwY"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this function\n",
    "def encode_and_align_labels(dataset, tokenizer, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Tokenizes the input tokens and aligns the word-level NER labels with the tokenized output.\"\"\"\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "\n",
    "    for example in dataset:\n",
    "        words = example[\"tokens\"]          # list of strings (words)\n",
    "        word_labels = example[\"ner_tags\"]  # list of ints (label ids) consistent with ner_tags dict\n",
    "\n",
    "        # Tokenize as a sequence of pre-split words\n",
    "        encoding = tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_sequence_length\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids() # from which original word each token position came from\n",
    "\n",
    "        aligned_labels = []\n",
    "        previous_word_id = None\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                # Special tokens ([CLS], [SEP]) (ignore in loss)\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_id != previous_word_id:\n",
    "                # First subword for a given word (keep the word-level label)\n",
    "                aligned_labels.append(word_labels[word_id])\n",
    "            else:\n",
    "                # Subsequent subword pieces (set to -100 so they're ignored by the loss)\n",
    "                aligned_labels.append(-100)\n",
    "\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        all_input_ids.append(encoding[\"input_ids\"])\n",
    "        all_attention_masks.append(encoding[\"attention_mask\"])\n",
    "        all_labels.append(aligned_labels)\n",
    "\n",
    "    # New dataset object containing only the encoded fields\n",
    "    encoded_dataset = datasets.Dataset.from_dict(\n",
    "        {\n",
    "            \"input_ids\": all_input_ids,\n",
    "            \"attention_mask\": all_attention_masks,\n",
    "            \"labels\": all_labels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return encoded_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6558,
     "status": "ok",
     "timestamp": 1763317279541,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "_0zufGtNw6yk"
   },
   "outputs": [],
   "source": [
    "# TODO: Encode the two training sets and the test set by applying the function above\n",
    "encoded_test_set = encode_and_align_labels(test_set, tokenizer, max_sequence_length)\n",
    "encoded_train_set1000 = encode_and_align_labels(train_set1000, tokenizer, max_sequence_length)\n",
    "encoded_train_set3000 = encode_and_align_labels(train_set3000, tokenizer, max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Set format for PyTorch\n",
    "encoded_test_set.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "encoded_train_set1000.set_format(\n",
    "\ttype=\"torch\",\n",
    "\tcolumns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "encoded_train_set3000.set_format(\n",
    "\ttype=\"torch\",\n",
    "\tcolumns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1763317279559,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "oykr25qgnBOg",
    "outputId": "2c4a95b0-91b7-493b-d7a5-b5db5e25ea26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([128])\n",
      "attention_mask: torch.Size([128])\n",
      "labels: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Check out how the training sets are encoded\n",
    "for key, val in encoded_train_set1000[0].items():\n",
    "    print(f'{key}: {val.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEJXYvbOa-E4"
   },
   "source": [
    "Example of how your output could look like.\n",
    "\n",
    "input_ids: torch.Size([128])\n",
    "\n",
    "token_type_ids: torch.Size([128])\n",
    "\n",
    "attention_mask: torch.Size([128])\n",
    "\n",
    "labels: torch.Size([128])\n",
    "\n",
    "üìù‚ùìWhat value should replace the three question marks in your print? Should this be the sample for all samples? Why/Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oZ-q_zoIxsG"
   },
   "source": [
    "\n",
    "- The three question marks should be replaced with the `max_sequence_length`\n",
    "- Yes, it will be the same for all samples because we used `padding=\"max_length\"` and therefore every sequence is padded/truncated to that fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnWH-MaKO5Q1"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdIPhpLiO5Q1"
   },
   "source": [
    "## Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 11083,
     "status": "ok",
     "timestamp": 1763317290635,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "sy2LxB_2O5Q2"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSpxJMtkO5Q3"
   },
   "source": [
    "**TODO: Complete the following, reusable functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763317290639,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "gKhJH6wQO5Q4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    \"\"\"\n",
    "    Compute macro and micro F1 scores from PredictionOutput\n",
    "\n",
    "    Args:\n",
    "        preds: transformers.trainer_utils.PredictionOutput\n",
    "\n",
    "    Returns:\n",
    "        dict with macro_f1 and micro_f1 scores\n",
    "    \"\"\"\n",
    "    logits = preds.predictions\n",
    "    label_ids = preds.label_ids\n",
    "\n",
    "    # Argmax over label dimension\n",
    "    y_pred = np.argmax(logits, axis=-1).ravel()\n",
    "    y_true = label_ids.ravel()\n",
    "\n",
    "    # Mask out ignored positions\n",
    "    valid = y_true != -100\n",
    "    y_true = y_true[valid]\n",
    "    y_pred = y_pred[valid]\n",
    "\n",
    "    # Exclude 'O' from scoring (optional but recommended for NER)\n",
    "    o_id = ner_tags[\"O\"]\n",
    "    keep = y_true != o_id\n",
    "    y_true = y_true[keep]\n",
    "    y_pred = y_pred[keep]\n",
    "\n",
    "    # Metrics\n",
    "    macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0) # zero_division=0 avoids warnings when a class is absent in predictions\n",
    "    micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0) # zero_division=0 avoids warnings when a class is absent in predictions\n",
    "\n",
    "    return {\"macro_f1\": macro, \"micro_f1\": micro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1763317290642,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "ePcLuf4QO5Q4"
   },
   "outputs": [],
   "source": [
    "def freeze_weights(model):\n",
    "    \"\"\"Freeze the weights for a given model.\n",
    "\n",
    "    Args:\n",
    "        model: transformers.PreTrainedModel\n",
    "\n",
    "    Returns:\n",
    "\t\t\tmodel: transformers.PreTrainedModel\n",
    "    \"\"\"\n",
    "    for name, param in model.bert.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cevovk8ZO5Q5"
   },
   "source": [
    "## Variation 1: 1000 sentences, no frozen weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru--p5NZO5Q6"
   },
   "source": [
    "**TODO: Initialise your model and set up your training arguments**\n",
    "\n",
    "üìù‚ùìWhen initializing the BertForTokenClassification-class with BERT-base you should get a warning message. Explain why you get this message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI_-AstTOF0U"
   },
   "source": [
    "\n",
    "Because the token-classification head (`classifier.weight` / `classifier.bias`) does not exist in the base checkpoint. When we call `from_pretrained(model_checkpoint, num_labels=7, ...)`, the library loads the BERT backbone weights from the checkpoint but initializes a new, randomly-initialized classification layer sized to the number of labels. Transformers prints a warning like: ‚ÄúSome weights of BertForTokenClassification were not initialized from the model checkpoint and are newly initialized: ['classifier.*']‚Äù, which is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1763317290644,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "S9VIPOT9ZH_7"
   },
   "outputs": [],
   "source": [
    "# id2label/label2id maps for the model\n",
    "label2id = ner_tags\n",
    "id2label = {v: k for k, v in ner_tags.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1763317290660,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "JD72g8GC2Rwx"
   },
   "outputs": [],
   "source": [
    "def train_and_eval(train_dataset, run_name, freeze=False, epochs=3, lr=5e-5,\n",
    "                   per_device_train_bs=8, per_device_eval_bs=8):\n",
    "    \"\"\"\n",
    "    This function trains a BERT token-classifier on a given dataset and evaluates on an encoded_test_set.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: Encoded HuggingFace Dataset\n",
    "        run_name: str-tag for output directory\n",
    "        freeze: bool, whether to freeze the BERT backbone\n",
    "        epochs: training epochs\n",
    "        lr: learning rate\n",
    "        per_device_train_bs: train batch size per device\n",
    "        per_device_eval_bs: eval batch size per device\n",
    "\n",
    "    Returns:\n",
    "        (trainer, metrics) where the metrics is a dictionary\n",
    "    \"\"\"\n",
    "    # 1. Load model with the correct number of labels + mapping\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    # 2. Freeze backbone\n",
    "    if freeze:\n",
    "        model = freeze_weights(model)\n",
    "\n",
    "    # 3. Training args\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./ner_{run_name}\",\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=per_device_train_bs,\n",
    "        per_device_eval_batch_size=per_device_eval_bs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=[],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=None\n",
    "    )\n",
    "\n",
    "    # 4. Train\n",
    "    trainer.train()\n",
    "\n",
    "    # 5. Evaluate on the (already-encoded) test set\n",
    "    pred_output = trainer.predict(encoded_test_set)\n",
    "    metrics = compute_metrics(pred_output)\n",
    "\n",
    "    print(f\"\\n=== {run_name} ===\")\n",
    "    print(metrics)\n",
    "\n",
    "    return trainer, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_DBflx1wzRa"
   },
   "source": [
    "**TODO: Train your Model ‚ö° GPU 2-3 mins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "executionInfo": {
     "elapsed": 54341,
     "status": "ok",
     "timestamp": 1763317345033,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "Z3XEgVHrxJTq",
    "outputId": "3edc5980-b2f7-4d0f-b378-f54d3aa1c12a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3098669530.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.980300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.095800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== es_1k_unfrozen ===\n",
      "{'macro_f1': 0.7533482511389789, 'micro_f1': 0.8697937727456531}\n"
     ]
    }
   ],
   "source": [
    "# Set-Up 1: 1000 sentences, no frozen weights\n",
    "trainer_1, metrics_1 = train_and_eval(\n",
    "    train_dataset=encoded_train_set1000,\n",
    "    run_name=\"es_1k_unfrozen\",\n",
    "    freeze=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt86VaDIiMWd"
   },
   "source": [
    "**TODO: Compute Metrics/Performance of your model.**\n",
    "\n",
    "üìù‚ùì Is there a challenge when evaluating the predictions of your model? Why is this challenge present and how do you plan to deal with it?\n",
    "\n",
    "Hint: Look at the lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMZ70S5aPnxB"
   },
   "source": [
    "\n",
    "Yes, we discovered two different challenges when evaluating the predictions of our model. One is the variable sequence lengths. The problem is that batches are padded to `max_sequence_length`, so predictions include positions that are just padding or special tokens. The second challenge is the word-piece tokenization. Our label alignment introduced `-100` for tokens we do not want to score. This means that during evaluation, we should mask out all positions where the gold label is `-100` before computing metrics. The `compute_metrics` above does exactly that. Additionally, we excluded the `'O'` label from F1, otherwise the overwhelming frequency of `'O'` can inflate scores and obscure entity performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ryi-Vnrw7NJ"
   },
   "source": [
    "To avoid rerunning, please also print the metrics of each model that completed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1763317345076,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "Pe2uyoOAcleD",
    "outputId": "a441cc76-e84e-4194-bac6-1aadd078151f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set-Up 1 metrics: {'macro_f1': 0.7533482511389789, 'micro_f1': 0.8697937727456531}\n"
     ]
    }
   ],
   "source": [
    "print(\"Set-Up 1 metrics:\", metrics_1)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AKh4cXm7F8R"
   },
   "source": [
    "## Variant 2: 3000 sentences, no frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "executionInfo": {
     "elapsed": 110216,
     "status": "ok",
     "timestamp": 1763317455290,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "HHmo4lCp2Bj7",
    "outputId": "b46ba39a-6f2b-4155-9cf5-30182dd4c68f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3098669530.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 01:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.334200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== es_3k_unfrozen ===\n",
      "{'macro_f1': 0.7700985109433344, 'micro_f1': 0.894190591723952}\n"
     ]
    }
   ],
   "source": [
    "# Set-Up 2: 3000 sentences, no frozen weights\n",
    "trainer_2, metrics_2 = train_and_eval(\n",
    "    train_dataset=encoded_train_set3000,\n",
    "    run_name=\"es_3k_unfrozen\",\n",
    "    freeze=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1763317455329,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "WoOlyXowboi5",
    "outputId": "7bc086b4-461f-45da-8e8e-f4e2bb3a66f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set-Up 2 metrics: {'macro_f1': 0.7700985109433344, 'micro_f1': 0.894190591723952}\n"
     ]
    }
   ],
   "source": [
    "print(\"Set-Up 2 metrics:\", metrics_2)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsU1E0_p7ITW"
   },
   "source": [
    "## Variant 3: 1000 sentences, frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "executionInfo": {
     "elapsed": 14286,
     "status": "ok",
     "timestamp": 1763317469637,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "FTXRQCgz7Vcu",
    "outputId": "c463705c-c66d-4cab-e7b3-5ca609122039"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3098669530.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.603700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== es_1k_frozen ===\n",
      "{'macro_f1': 0.1292023844578494, 'micro_f1': 0.24814665049198006}\n"
     ]
    }
   ],
   "source": [
    "# Set-Up 3: 1000 sentences, frozen weights\n",
    "trainer_3, metrics_3 = train_and_eval(\n",
    "    train_dataset=encoded_train_set1000,\n",
    "    run_name=\"es_1k_frozen\",\n",
    "    freeze=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1763317469657,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "oXKm2GLXbvoz",
    "outputId": "1c3c34e5-57c4-4678-ba50-6cd7f03dd2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set-Up 3 metrics: {'macro_f1': 0.1292023844578494, 'micro_f1': 0.24814665049198006}\n"
     ]
    }
   ],
   "source": [
    "print(\"Set-Up 3 metrics:\", metrics_3)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6wE6mAm7y7m"
   },
   "source": [
    "## Variant 4: 3000 sentences, frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "executionInfo": {
     "elapsed": 31166,
     "status": "ok",
     "timestamp": 1763317500824,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "RxkAuigt7y7n",
    "outputId": "db08c4bd-9f45-4833-90e5-926ce1369434"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3098669530.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.534800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.390900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.353400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.249900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.251100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== es_3k_frozen ===\n",
      "{'macro_f1': 0.28391817404275826, 'micro_f1': 0.3640652379026823}\n"
     ]
    }
   ],
   "source": [
    "# Set-Up 4: 3000 sentences, frozen weights\n",
    "trainer_4, metrics_4 = train_and_eval(\n",
    "    train_dataset=encoded_train_set3000,\n",
    "    run_name=\"es_3k_frozen\",\n",
    "    freeze=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1763317500853,
     "user": {
      "displayName": "Natalia",
      "userId": "17234517296922487844"
     },
     "user_tz": -60
    },
    "id": "7wJgpMGtb22n",
    "outputId": "baa11df9-ae24-404f-c9af-3cdc37792911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set_up 4 metrics: {'macro_f1': 0.28391817404275826, 'micro_f1': 0.3640652379026823}\n"
     ]
    }
   ],
   "source": [
    "print(\"Set_up 4 metrics:\", metrics_4)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iTcwgmtO5Q_"
   },
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwR6hIBqyJri"
   },
   "source": [
    "### All the questions from between the code blocks:\n",
    " üìù‚ùìWhy do you need to be aware of the longest input length within your dataset? Which parameter of the model dictates this?  \n",
    "\n",
    " We must be aware of the longest input length because BERT has a fixed maximum context window. Anything longer must be truncated (information loss) or split across chunks. The limit is set by the model‚Äôs configuration parameter `config.max_position_embeddings` (exposed via the tokenizer as `tokenizer.model_max_length`). Choosing an appropriate `max_sequence_length` ensures that we do not exceed this limit.\n",
    "\n",
    " üìù‚ùìThe dataset is split into words, and the assigned labels are for words. How should we deal with labels **after** tokenization? NOTE: Each word may be split into one or multiple tokens by the tokenizer.  \n",
    "\n",
    "The dataset labels are at word-level. After tokenization, each word might produce multiple subword tokens. The standard and simplest approach for BertForTokenClassification is:\n",
    "\n",
    "1. Assign the original word label to the first subword token of that word.\n",
    "2. Assign -100 to all subsequent subword pieces and special tokens ([CLS], [SEP]).\n",
    "    - -100 is the default ignore index in PyTorch‚Äôs CrossEntropyLoss, so those positions don‚Äôt affect the loss (they are ignored).\n",
    "\n",
    "This preserves alignment while ensuring the loss is computed once per original word.\n",
    "\n",
    "üìù‚ùìWhat value should replace the three question marks in your print? Should this be the sample for all samples? Why/Why not?  \n",
    "- The three question marks should be replaced with the `max_sequence_length`\n",
    "- Yes, it will be the same for all samples because we used `padding=\"max_length\"` and therefore every sequence is padded/truncated to that fixed length.\n",
    "\n",
    "üìù‚ùìWhen initializing the BertForTokenClassification-class with BERT-base you should get a warning message. Explain why you get this message.  \n",
    "Because the token-classification head (`classifier.weight` / `classifier.bias`) does not exist in the base checkpoint. When we call `from_pretrained(model_checkpoint, num_labels=7, ...)`, the library loads the BERT backbone weights from the checkpoint but initializes a new, randomly-initialized classification layer sized to the number of labels. Transformers prints a warning like: ‚ÄúSome weights of BertForTokenClassification were not initialized from the model checkpoint and are newly initialized: ['classifier.*']‚Äù, which is expected.\n",
    "\n",
    "üìù‚ùì Is there a challenge when evaluating the predictions of your model? Why is this challenge present and how do you plan to deal with it?  \n",
    "Yes, we discovered two different challenges when evaluating the predictions of our model. One is the variable sequence lengths. The problem is that batches are padded to `max_sequence_length`, so predictions include positions that are just padding or special tokens. The second challenge is the word-piece tokenization. Our label alignment introduced `-100` for tokens we do not want to score. This means that during evaluation, we should mask out all positions where the label is `-100` before computing metrics. The `compute_metrics` above does exactly that. Additionally, we excluded the `'O'` label from F1, otherwise the overwhelming frequency of `'O'` can inflate scores and obscure entity performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2LyNOP6oVsz"
   },
   "source": [
    "üìù‚ùì Summary of Performance of the four Model Variants:\n",
    "\n",
    "1. Whole Model finetuning, 1000 samples:\\\n",
    "The model performs well even with limited data:\\\n",
    "**macro-F1 = 0.7533, micro-F1 = 0.8698**\\\n",
    "This shows that BERT‚Äôs pretrained layers adapt effectively to the WikiANN Spanish NER tags when allowed to update.\n",
    "2. Whole Model finetuning, 3000 samples:\\\n",
    "Performance improves even further:\\\n",
    "**macro-F1 = 0.7701, micro-F1 = 0.8942**\\\n",
    "The larger training set increases class coverage and improves generalization.\n",
    "3. Frozen Backbone, 1000 samples:\\\n",
    "Performance drops dramatically:\\\n",
    "**macro-F1 = 0.1292, micro-F1 = 0.2481**\\\n",
    "With frozen transformer weights, only the classification head learns. With only 1000 examples, this is insufficient to map contextualized embeddings to NER tags.\n",
    "4. Frozen Backbone 3000 samples:\\\n",
    "Still significantly underperforms compared to unfrozen training:\\\n",
    "**macro-F1 = 0.2839, micro-F1 = 0.3641**\\\n",
    "The extra data helps, but the model cannot update its contextual representations, so the improvement is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPi_aox2O5RA"
   },
   "source": [
    "üìù‚ùì When we freeze the transformer backbone weights, which weights are being tuned during fine-tuning?\n",
    "\n",
    "When the backbone is frozen, all transformer layers (embeddings + 12 encoder blocks) remain fixed.\\\n",
    "The only weights that continue to be trained are:\n",
    "- The token-classification head (a linear layer mapping hidden states to NER tag logits)\n",
    "- Any additional classification dropout layers attached to the head\n",
    "\n",
    "Thus, the model can only adjust the final mapping from contextual embeddings to tag predictions, but cannot update how tokens are contextualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR2j40qivASb"
   },
   "source": [
    "üìù‚ùì Are there differences between f1-micro and f1-macro score? If so, why?\n",
    "\n",
    "F1-micro is always larger than F1-macro. The difference is larger for the unfrozen models. This is because F1-micro aggregates all predictions across the entire dataset and is dominated by frequent classes, particularly, ‚ÄúO‚Äù (non-entity), which usually represents 70‚Äì90% of all tokens. F1-macro averages over all classes equally, giving rare classes (PER, ORG, LOC) the same weight as frequent ones. Therefore, a model can achieve high F1-micro even if it performs poorly on rare entity types, but F1-macro penalizes poor performance on minority classes, revealing weaknesses.\n",
    "\n",
    "The frozen models perform especially poorly on real entity tags, so F1-macro collapses, while F1-micro stays somewhat higher due to correct predictions on frequent `'O'` tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqyOEWsr-J4s"
   },
   "source": [
    "üìù‚ùì Is it better to freeze or not to freeze the transformer backbone weights? Hypothesize why\n",
    "\n",
    "Based on our results we can conclude that unfrozen models massively outperform frozen models. We believe that NER requires task-specific contextualization, meaning the model needs to update its internal representation of words in the context of Spanish NER. Freezing prevents learning specialized patterns such as multi-token names, organization/entity boundaries, capitalization cues and Spanish-specific morphology. With only the classification head being trainable, the model cannot adjust token embeddings, it cannot learn new contextual patterns and performance collapses, especially for minority classes (seen in F1-macro). This is why full fine-tuning is clearly superior for token-classification tasks like NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYGqUjRwyJrk"
   },
   "source": [
    "**Use of generative AI disclaimer**\n",
    "\n",
    "ChatGPT was used to assist in understanding certain parts of the existing code and to help generate new code snippets, which were then manually checked and corrected. Additionally, it was used for debugging purposes (explaining error messages and suggesting possible solutions)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
